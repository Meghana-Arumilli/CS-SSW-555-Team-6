{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "88e3d8a4",
      "metadata": {
        "id": "88e3d8a4"
      },
      "source": [
        "# EEG DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78e016ab",
      "metadata": {
        "id": "78e016ab"
      },
      "source": [
        "The dimensions of the training set are as follows: 4,500 samples, 64 channels, and a time length of 795. This corresponds to 5 categories in y_train.\n",
        "\n",
        "The dimensions of the testing set are as follows: 750 samples, 64 channels, and a time length of 795. This corresponds to 5 categories in y_test.\n",
        "\n",
        "You can download it from this Google Drive link: [https://drive.google.com/drive/folders/1ykR-mn4d4KfFeeNrfR6UdtebsNRY8PU2?usp=sharing].\n",
        "Please download the data and place it in your data_path at \"./data.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b284602d",
      "metadata": {
        "id": "b284602d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "GVVtF9J28Eyx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVVtF9J28Eyx",
        "outputId": "d102f315-fcdc-4bd9-bd5a-b0c19d56b064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fef2ddad",
      "metadata": {
        "id": "fef2ddad"
      },
      "outputs": [],
      "source": [
        "data_path =  '/content/drive/MyDrive/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fbb56c77",
      "metadata": {
        "id": "fbb56c77"
      },
      "outputs": [],
      "source": [
        "train_data = np.load(data_path + 'train_data.npy')\n",
        "test_data = np.load(data_path + 'test_data.npy')\n",
        "train_label = np.load(data_path + 'train_label.npy')\n",
        "test_label = np.load(data_path + 'test_label.npy')\n",
        "\n",
        "#To convert the data into PyTorch tensors\n",
        "x_train_tensor = torch.Tensor(train_data)\n",
        "y_train_tensor = torch.LongTensor(train_label)\n",
        "x_test_tensor = torch.Tensor(test_data)\n",
        "y_test_tensor = torch.LongTensor(test_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f13109bc",
      "metadata": {
        "id": "f13109bc"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #Setting GPU on your computer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "38430b1f",
      "metadata": {
        "id": "38430b1f"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(x_train_tensor.to(device), y_train_tensor.to(device)) # input data to Tensor dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, drop_last=True, shuffle=True) #  Batch size refers to the number of data sample\n",
        "test_dataset = TensorDataset(x_test_tensor.to(device), y_test_tensor.to(device))\n",
        "test_loader = DataLoader(test_dataset, batch_size=64,  drop_last=True,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a304b4d6",
      "metadata": {
        "id": "a304b4d6"
      },
      "source": [
        "# Build simple Deep learning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2f9468f8",
      "metadata": {
        "id": "2f9468f8"
      },
      "outputs": [],
      "source": [
        "class EEGAutoencoderClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(EEGAutoencoderClassifier, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(64 * 795, 256), # Input dimention is 64 channel * 795 time point, and use 256 units for first NN layer\n",
        "            nn.ReLU(), # Use ReLu function for NN training\n",
        "            nn.Linear(256, 128), # 256 NN units to 128 units\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),#  128 NN units to 64 units\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64, num_classes), # num_classes is 5 (hello,” “help me,” “stop,” “thank you,” and “yes”)\n",
        "            nn.LogSoftmax(dim=1)  # Use LogSoftmax for multi-class classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        # import pdb;pdb.set_trace()\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2369fa9c",
      "metadata": {
        "id": "2369fa9c"
      },
      "outputs": [],
      "source": [
        "num_classes = 5 # setting final output class\n",
        "model = EEGAutoencoderClassifier(num_classes).to(device)\n",
        "criterion = nn.NLLLoss() # Use NLLLoss function to optimize\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Setting parameters learning rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "086a7e33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "086a7e33",
        "outputId": "3a0b51ca-9036-43b1-e801-b9a95ce9a955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200, Loss: 1.5460231304168701\n",
            "Epoch 2/200, Loss: 1.4746407270431519\n",
            "Epoch 3/200, Loss: 1.4618765115737915\n",
            "Epoch 4/200, Loss: 1.405974268913269\n",
            "Epoch 5/200, Loss: 1.2254456281661987\n",
            "Epoch 6/200, Loss: 1.0531904697418213\n",
            "Epoch 7/200, Loss: 1.1934298276901245\n",
            "Epoch 8/200, Loss: 1.0638296604156494\n",
            "Epoch 9/200, Loss: 0.7753558158874512\n",
            "Epoch 10/200, Loss: 0.5620149374008179\n",
            "Epoch 11/200, Loss: 0.5787872672080994\n",
            "Epoch 12/200, Loss: 0.5097058415412903\n",
            "Epoch 13/200, Loss: 0.31779080629348755\n",
            "Epoch 14/200, Loss: 0.37457960844039917\n",
            "Epoch 15/200, Loss: 0.36143070459365845\n",
            "Epoch 16/200, Loss: 0.5005438923835754\n",
            "Epoch 17/200, Loss: 0.13512133061885834\n",
            "Epoch 18/200, Loss: 0.358467698097229\n",
            "Epoch 19/200, Loss: 0.2624067962169647\n",
            "Epoch 20/200, Loss: 0.34161293506622314\n",
            "Epoch 21/200, Loss: 0.2708033323287964\n",
            "Epoch 22/200, Loss: 0.09066638350486755\n",
            "Epoch 23/200, Loss: 0.171726256608963\n",
            "Epoch 24/200, Loss: 0.3261389434337616\n",
            "Epoch 25/200, Loss: 0.12000444531440735\n",
            "Epoch 26/200, Loss: 0.17198684811592102\n",
            "Epoch 27/200, Loss: 0.26469993591308594\n",
            "Epoch 28/200, Loss: 0.34580084681510925\n",
            "Epoch 29/200, Loss: 0.31302425265312195\n",
            "Epoch 30/200, Loss: 0.2571662366390228\n",
            "Epoch 31/200, Loss: 0.09832032024860382\n",
            "Epoch 32/200, Loss: 0.22631584107875824\n",
            "Epoch 33/200, Loss: 0.13127018511295319\n",
            "Epoch 34/200, Loss: 0.2528547942638397\n",
            "Epoch 35/200, Loss: 0.14518800377845764\n",
            "Epoch 36/200, Loss: 0.2690776586532593\n",
            "Epoch 37/200, Loss: 0.3645695745944977\n",
            "Epoch 38/200, Loss: 0.22846175730228424\n",
            "Epoch 39/200, Loss: 0.1697903871536255\n",
            "Epoch 40/200, Loss: 0.08193062990903854\n",
            "Epoch 41/200, Loss: 0.12619255483150482\n",
            "Epoch 42/200, Loss: 0.2064637839794159\n",
            "Epoch 43/200, Loss: 0.27059879899024963\n",
            "Epoch 44/200, Loss: 0.04500453919172287\n",
            "Epoch 45/200, Loss: 0.03980128839612007\n",
            "Epoch 46/200, Loss: 0.06450442969799042\n",
            "Epoch 47/200, Loss: 0.008936691097915173\n",
            "Epoch 48/200, Loss: 0.003978975582867861\n",
            "Epoch 49/200, Loss: 0.14696361124515533\n",
            "Epoch 50/200, Loss: 0.19354812800884247\n",
            "Epoch 51/200, Loss: 0.37309643626213074\n",
            "Epoch 52/200, Loss: 0.3061569929122925\n",
            "Epoch 53/200, Loss: 0.20947755873203278\n",
            "Epoch 54/200, Loss: 0.2036035805940628\n",
            "Epoch 55/200, Loss: 0.06116803362965584\n",
            "Epoch 56/200, Loss: 0.0511741004884243\n",
            "Epoch 57/200, Loss: 0.08386414498090744\n",
            "Epoch 58/200, Loss: 0.06286927312612534\n",
            "Epoch 59/200, Loss: 0.11145581305027008\n",
            "Epoch 60/200, Loss: 0.19563591480255127\n",
            "Epoch 61/200, Loss: 0.22475796937942505\n",
            "Epoch 62/200, Loss: 0.07082190364599228\n",
            "Epoch 63/200, Loss: 0.1202198714017868\n",
            "Epoch 64/200, Loss: 0.07993901520967484\n",
            "Epoch 65/200, Loss: 0.029685290530323982\n",
            "Epoch 66/200, Loss: 0.07138682901859283\n",
            "Epoch 67/200, Loss: 0.10755100846290588\n",
            "Epoch 68/200, Loss: 0.22068123519420624\n",
            "Epoch 69/200, Loss: 0.029316313564777374\n",
            "Epoch 70/200, Loss: 0.025416549295186996\n",
            "Epoch 71/200, Loss: 0.010166830383241177\n",
            "Epoch 72/200, Loss: 0.006961582228541374\n",
            "Epoch 73/200, Loss: 0.08445009589195251\n",
            "Epoch 74/200, Loss: 0.08557314425706863\n",
            "Epoch 75/200, Loss: 0.09047769010066986\n",
            "Epoch 76/200, Loss: 0.025876065716147423\n",
            "Epoch 77/200, Loss: 0.37427666783332825\n",
            "Epoch 78/200, Loss: 0.12774881720542908\n",
            "Epoch 79/200, Loss: 0.1465388387441635\n",
            "Epoch 80/200, Loss: 0.08559098094701767\n",
            "Epoch 81/200, Loss: 0.24276861548423767\n",
            "Epoch 82/200, Loss: 0.03846808895468712\n",
            "Epoch 83/200, Loss: 0.13234467804431915\n",
            "Epoch 84/200, Loss: 0.01257492508739233\n",
            "Epoch 85/200, Loss: 0.05224251002073288\n",
            "Epoch 86/200, Loss: 0.04587344825267792\n",
            "Epoch 87/200, Loss: 0.014772927388548851\n",
            "Epoch 88/200, Loss: 0.15863804519176483\n",
            "Epoch 89/200, Loss: 0.2314179390668869\n",
            "Epoch 90/200, Loss: 0.469253271818161\n",
            "Epoch 91/200, Loss: 0.11349926888942719\n",
            "Epoch 92/200, Loss: 0.022811563685536385\n",
            "Epoch 93/200, Loss: 0.01037844642996788\n",
            "Epoch 94/200, Loss: 0.031899407505989075\n",
            "Epoch 95/200, Loss: 0.04517510533332825\n",
            "Epoch 96/200, Loss: 0.005350105464458466\n",
            "Epoch 97/200, Loss: 0.010673650540411472\n",
            "Epoch 98/200, Loss: 0.0033157914876937866\n",
            "Epoch 99/200, Loss: 0.372639536857605\n",
            "Epoch 100/200, Loss: 0.2836812138557434\n",
            "Epoch 101/200, Loss: 0.14047420024871826\n",
            "Epoch 102/200, Loss: 0.32399091124534607\n",
            "Epoch 103/200, Loss: 0.00861323531717062\n",
            "Epoch 104/200, Loss: 0.06180472671985626\n",
            "Epoch 105/200, Loss: 0.02177080698311329\n",
            "Epoch 106/200, Loss: 0.02923666685819626\n",
            "Epoch 107/200, Loss: 0.011225907132029533\n",
            "Epoch 108/200, Loss: 0.06958267837762833\n",
            "Epoch 109/200, Loss: 0.01868285797536373\n",
            "Epoch 110/200, Loss: 0.12705309689044952\n",
            "Epoch 111/200, Loss: 0.2672826051712036\n",
            "Epoch 112/200, Loss: 0.03391352668404579\n",
            "Epoch 113/200, Loss: 0.020487183704972267\n",
            "Epoch 114/200, Loss: 0.010782470926642418\n",
            "Epoch 115/200, Loss: 0.06345459818840027\n",
            "Epoch 116/200, Loss: 0.06959845870733261\n",
            "Epoch 117/200, Loss: 0.06573696434497833\n",
            "Epoch 118/200, Loss: 0.023928524926304817\n",
            "Epoch 119/200, Loss: 0.06183716654777527\n",
            "Epoch 120/200, Loss: 0.1529574692249298\n",
            "Epoch 121/200, Loss: 0.1388026773929596\n",
            "Epoch 122/200, Loss: 0.026413489133119583\n",
            "Epoch 123/200, Loss: 0.013714268803596497\n",
            "Epoch 124/200, Loss: 0.026880383491516113\n",
            "Epoch 125/200, Loss: 0.03491813316941261\n",
            "Epoch 126/200, Loss: 0.0008303386275656521\n",
            "Epoch 127/200, Loss: 0.0020657835993915796\n",
            "Epoch 128/200, Loss: 0.01764017529785633\n",
            "Epoch 129/200, Loss: 0.0014863034011796117\n",
            "Epoch 130/200, Loss: 0.0037547722458839417\n",
            "Epoch 131/200, Loss: 0.01088650431483984\n",
            "Epoch 132/200, Loss: 0.00406657624989748\n",
            "Epoch 133/200, Loss: 0.1329183578491211\n",
            "Epoch 134/200, Loss: 0.03837094455957413\n",
            "Epoch 135/200, Loss: 0.19739346206188202\n",
            "Epoch 136/200, Loss: 0.021144624799489975\n",
            "Epoch 137/200, Loss: 0.058869145810604095\n",
            "Epoch 138/200, Loss: 0.005616141948848963\n",
            "Epoch 139/200, Loss: 0.23430880904197693\n",
            "Epoch 140/200, Loss: 0.10727196931838989\n",
            "Epoch 141/200, Loss: 0.1554916501045227\n",
            "Epoch 142/200, Loss: 0.025932401418685913\n",
            "Epoch 143/200, Loss: 0.006193626672029495\n",
            "Epoch 144/200, Loss: 0.003777834353968501\n",
            "Epoch 145/200, Loss: 0.20943011343479156\n",
            "Epoch 146/200, Loss: 0.0065826596692204475\n",
            "Epoch 147/200, Loss: 0.04520653560757637\n",
            "Epoch 148/200, Loss: 0.07949897646903992\n",
            "Epoch 149/200, Loss: 0.11767970770597458\n",
            "Epoch 150/200, Loss: 0.10262755304574966\n",
            "Epoch 151/200, Loss: 0.10362445563077927\n",
            "Epoch 152/200, Loss: 0.00319291721098125\n",
            "Epoch 153/200, Loss: 0.044007204473018646\n",
            "Epoch 154/200, Loss: 0.006818684749305248\n",
            "Epoch 155/200, Loss: 0.005066734738647938\n",
            "Epoch 156/200, Loss: 0.010173085145652294\n",
            "Epoch 157/200, Loss: 0.28033435344696045\n",
            "Epoch 158/200, Loss: 0.08358905464410782\n",
            "Epoch 159/200, Loss: 0.06841865181922913\n",
            "Epoch 160/200, Loss: 0.033612340688705444\n",
            "Epoch 161/200, Loss: 0.21270574629306793\n",
            "Epoch 162/200, Loss: 0.30377355217933655\n",
            "Epoch 163/200, Loss: 0.26123347878456116\n",
            "Epoch 164/200, Loss: 0.06593698263168335\n",
            "Epoch 165/200, Loss: 0.05727706104516983\n",
            "Epoch 166/200, Loss: 0.20577730238437653\n",
            "Epoch 167/200, Loss: 0.014414661563932896\n",
            "Epoch 168/200, Loss: 0.023813914507627487\n",
            "Epoch 169/200, Loss: 0.06864295899868011\n",
            "Epoch 170/200, Loss: 0.008288662880659103\n",
            "Epoch 171/200, Loss: 0.010418996214866638\n",
            "Epoch 172/200, Loss: 0.08736313134431839\n",
            "Epoch 173/200, Loss: 0.18435196578502655\n",
            "Epoch 174/200, Loss: 0.004927271045744419\n",
            "Epoch 175/200, Loss: 0.05025120824575424\n",
            "Epoch 176/200, Loss: 0.00044806540245190263\n",
            "Epoch 177/200, Loss: 0.04224229231476784\n",
            "Epoch 178/200, Loss: 0.0009225940448231995\n",
            "Epoch 179/200, Loss: 0.0019162842072546482\n",
            "Epoch 180/200, Loss: 0.11467503756284714\n",
            "Epoch 181/200, Loss: 0.002543460810557008\n",
            "Epoch 182/200, Loss: 0.016511432826519012\n",
            "Epoch 183/200, Loss: 0.017848040908575058\n",
            "Epoch 184/200, Loss: 0.212590754032135\n",
            "Epoch 185/200, Loss: 0.13485930860042572\n",
            "Epoch 186/200, Loss: 0.06130583956837654\n",
            "Epoch 187/200, Loss: 0.011680813506245613\n",
            "Epoch 188/200, Loss: 0.05944608524441719\n",
            "Epoch 189/200, Loss: 0.01637152023613453\n",
            "Epoch 190/200, Loss: 0.017926085740327835\n",
            "Epoch 191/200, Loss: 0.03721586614847183\n",
            "Epoch 192/200, Loss: 0.1307707279920578\n",
            "Epoch 193/200, Loss: 0.03496157005429268\n",
            "Epoch 194/200, Loss: 0.00505930045619607\n",
            "Epoch 195/200, Loss: 0.12560753524303436\n",
            "Epoch 196/200, Loss: 0.22279250621795654\n",
            "Epoch 197/200, Loss: 0.085604228079319\n",
            "Epoch 198/200, Loss: 0.08082187175750732\n",
            "Epoch 199/200, Loss: 0.0005889653693884611\n",
            "Epoch 200/200, Loss: 0.12133461236953735\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 200 # setting training epochs (Number of training iterations)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for data, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7969f355",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7969f355",
        "outputId": "163e388e-5844-4bdc-caa8-a5d9700bb235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 49.15%\n"
          ]
        }
      ],
      "source": [
        "model.eval() # Evaluate your model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4jV7-MAnQm3w",
      "metadata": {
        "id": "4jV7-MAnQm3w"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "nEcVPP_lQ9FA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEcVPP_lQ9FA",
        "outputId": "c7f9dfb2-7a2a-4edb-dd4c-6409c60bc2b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Saved Models/BCI_200']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "joblib.dump(model,'/content/drive/MyDrive/Saved Models/BCI_200')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
